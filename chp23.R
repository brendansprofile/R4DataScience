# https://r4ds.had.co.nz/model-intro.html
# https://r4ds.had.co.nz/model-basics.html


# -------------------------------------------------------------------------

# CHP 22 INTRO

#The goal of a model is to provide a simple low-dimensional summary of a dataset.
  # Ideally, the model will capture true "signals" (i.e. patterns generated by the 
  # phenomenon of interest), and ignore "noise" (i.e. random variation that you're 
  # not interested in).


# 22.1 HYPOTHESIS GENERATION vs HYPOTHESIS TESTING

# Traditionally focus of models on inference or confirming a hypothesis is true

# Pair of ideas to do inference correctly 
  # 1) each observation can either be used for exploration or confirmation, NOT BOTH!
  # 2) You can use an observation as many times as you like for exploration,
    # but you can only use it once for confirmation.

# This is necessary because to confirm a hypothesis you must use 
# data independent of the data that you used to generate the hypothesis


# TIP:
  # If serious about doing CONFIRMATORY analysis, split data into 3 pieces before
  
# 1. 60% of your data goes into a training (or exploration) set. You're allowed 
# to do anything you like with this data: visualise it and fit tons of models to it.

# 2 . 20% goes into a query set. You can use this data to compare models or 
# visualisations by hand, but you're not allowed to use it as part of an 
# automated process.

# 3. 20% is held back for a test set. You can only use this data ONCE, 
# to test your final model.




# -------------------------------------------------------------------------

# CHP 23 MODEL BASICS

# Two parts to a model...
  # First, define a family of models that express precise, but generic pattern
  # you want to follow (ex: y = a_1*x^2 )

  # Next you generate a fitted model by finding the model from the family that 
  # is closes to your data (ex: y = 9x^2) ... MAKES GENERAL SPECIFIC

#NOTE: NOT "Is the model true?" ... BUT "Is the model illuminating and useful?".

#NOTE: goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.


library(tidyverse)

library(modelr) # base R's modelling functions
options(na.action = na.warn)



# -------------------------------------------------------------------------

# 23.2 SIMPLE MODEL

sim1

ggplot(sim1, aes(x,y)) +
  geom_point() # relationship looks linear like y = a0 + a1*x

# let's overlay diff lines w geom_abline to get a feel

models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x,y)) +
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()

# look at the vertical dist between each point and the model
# distance = difference y(prediction) - y(actual "response")

model1 <- function(a,data) {
  a[1] + data$x * a[2]
}

model1(c(7,1.5),sim1)

# rmse
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
#> [1] 2.665212

sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

# purrr to comptue distance for above models
models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models

# overlay 10 best models
ggplot(sim1, aes(x,y)) +
  geom_point(size = 2, colour = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist),
    data = filter(models, rank(dist) <= 10)
  )
  # coloured the models by -dist: this is an easy way to make sure that the best 
  # models (i.e. the ones with the smallest distance) get the brighest colours.


# can also think of these models as observations 
ggplot(models, aes(a1,a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

# grid search
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 

#  overlay the best 10 models back on the original data
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )

# better method = minimization tool 
# Newton Raphson search
best <- optim(c(0,0), measure_distance, data = sim1)
best$par
#[1] 4.222248 2.051204

ggplot(sim1, aes(x,y)) +
  geom_point(size = 2, colour = "grey30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])

# r has tool specifically to fit linear model
  # --> lm()
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
  # (Intercept)           x 
  # 4.220822    2.051533 


# EXERCISES
  # 1 - Fit a linear model to the simulated data below, and visualise the results. 
  # Rerun a few times to generate different simulated datasets. What do you notice 
  # about the model?
sim1a <- tibble(
  x = rep(1:10, each = 3), 
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
sim1a %>% 
  ggplot(aes(x,y))+
  geom_point()

sim1a_mod <- lm(y~x, data = sim1a)
sim1a_coef <- coef(sim1a_mod)
# (Intercept)           x 
# 5.296338    1.574857 
ggplot(sim1a,aes(x,y))+
  geom_point(size = 2, colour = "red") +
  geom_abline(intercept = sim1a_coef[1], slope = sim1a_coef[2])

# OR ...
ggplot(sim1a, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)


  # 2 - ...



  # 3 - 

# 23.3 VISUALIZING MODELS -------------------------------------------------

# you can figure out what pattern the model captures by carefully studying the
  # model family and the fitted coefficients....

# **** We're going to focus on understanding a model by looking at its predictions. 

# Residuals are powerful because they allow us to use models to remove striking 
# patterns so we can study the subtler trends that remain.


# generating an evenly spaced grid of values that covers the region where our data lies
grid <- sim1 %>% 
  data_grid(x)   #modelr::data_grid()
grid


# takes a data frame and a model. It adds the predictions from the model to a new column in the data frame:
grid <- grid %>% 
  add_predictions(sim1_mod)   #modelr::add_predictions()
grid

# this method allows to visualize any model
ggplot(sim1, aes(x))+
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)


## RESIDUALS------------------------
  # flip of predictions... RESIDUALS = what model missed (dist between obs & predicted)

sim1 <- sim1 %>% 
  add_residuals(sim1_mod)
sim1

# one way to understand residuals is with a frequency plot
ggplot(sim1,aes(resid)) +
  geom_freqpoly(binwidth = 0.5)

# recreate plots using residuals... should look random
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 



# EXERCISES
# 1  - loess() fits a smoothed curve, repeat process on sim1
sim1

# model both loess and lm
sim1_loess <- loess(y ~ x, data = sim1)
sim1_lm <- lm(y ~ x, data = sim1)

# add preds to grid
grid_loess <- sim1 %>% 
  add_predictions(sim1_loess)

# add all to sim1
sim1 <- sim1 %>% 
  add_predictions(sim1_lm) %>% 
  add_residuals(sim1_lm) %>% 
  add_predictions(sim1_loess, var = "pred_loess") %>% 
  add_residuals(sim1_loess, var = "resid_loess")

#plot sim 1, overlay line data = grid_loess
plot_sim1_loess <- 
  ggplot(sim1, aes(x, y)) + 
  geom_point() +
  geom_line(aes(x, pred), data = grid_loess, colour = "red")
plot_sim1_loess

sim1 %>% 
  ggplot(aes(x,y)) +
  geom_point() +
  geom_smooth(se = FALSE)
# `geom_smooth()` using method = 'loess' and formula 'y ~ x'

  # *** SAME --M geom_smooth() uses loess as default !!!!
  

# 2 - 
  # gather_predictions() & spread_predictions() allow to add  predictions
  # from multiple models at once

# GATHER_PREDICTIONS() - add column and stack rows with the model name
grid %>% 
  gather_predictions(sim1_lm, sim1_loess)
    # # A tibble: 20 x 3
    # model          x  pred
    # <chr>      <int> <dbl>
    #   1 sim1_lm        1  6.27
    # 2 sim1_lm        2  8.32
    # 3 sim1_lm        3 10.4 
    # # .....
    # 19 sim1_loess     9 22.6 
    # 20 sim1_loess    10 24.0 

# SPREAD_PREDICTIONS() - add multiple columns for each model
grid %>% 
  spread_predictions(sim1_lm, sim1_loess)
    # # A tibble: 10 x 4
    # x  pred sim1_lm sim1_loess
    # <int> <dbl>   <dbl>      <dbl>
    #   1     1  6.27    6.27       5.34
    # 2     2  8.32    8.32       8.27
    # 3     3 10.4    10.4       10.8 


# 3- 
geom_ref_line() # adds a reference line ?

# 4 - ...





# 23.4 FORMULAS AND MODELS ------------------------------------------------

# ..

df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)

model_matrix(df, y ~ x1)
  # # A tibble: 2 x 2
  #   `(Intercept)`    x1
  #     <dbl> <dbl>
  #   1             1     2
  #   2             1     1

#  R adds the intercept to the model is just by having a column that is full of ones. By default, R will always add this column. 
  # --> to drop it add " -1"
model_matrix(df, y ~ x1 -1)
  # # A tibble: 2 x 1
  # x1
  # <dbl>
  # 1     2
  # 2     1

model_matrix(df, y ~ x1 + x2)   # ADD MORE VARS
  # # A tibble: 2 x 3
  #   `(Intercept)`    x1    x2
  #   <dbl> <dbl> <dbl>
  #   1             1     2     5
  #   2             1     1     6


# CATEGORICAL VARIABLES
  # to use categorical models R must convert to a binary variable
df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1
)

# R does is convert it to y = x_0 + x_1 * sexmale
model_matrix(df, response ~ sex)
    # # A tibble: 3 x 2
    # `       (Intercept)`  sexmale
    # <dbl>   <dbl>
    #   1             1       1
    #   2             1       0


# new dataset
ggplot(sim2) +
  geom_point(aes(x,y))

# fit a model and generate predictions
mod2 <- lm(y ~ x, data = sim2) #...effectively predicting mean value for each x

grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
      # # A tibble: 4 x 2
      #   x      pred
      # <chr> <dbl>
      # 1 a      1.15
      # 2 b      8.12
      # 3 c      6.13
      # 4 d      1.91

# overlay predictions to see it shows a mean for each category (a...d)
ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size  = 4)


### INTERACTIONS (CONT & CATEG) --------------------

  # what happens when you combine categorical w/ continuous
ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(colour = x2))

# two models options to fit to data
mod1 <- lm(y ~ x1 + x2, data = sim3) # (+) model assumes vars independent
mod2 <- lm(y ~ x1 * x2, data = sim3) # (*) fits with interactions

# to visualize need 2 new tricks
  # 1. need to give data_grid both variables
  # 2. need to use gather_predictions() OR spread_predictions()

grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
    # # A tibble: 80 x 4
    # model    x1 x2     pred
    # <chr> <int> <fct> <dbl>
    # 1 mod1      1 a      1.67 .....

# visualize both with facets
ggplot(sim3, aes(x1, y, colour = x2)) + 
  geom_point() +
  geom_line(data = grid, aes(y = pred)) +
  facet_wrap(~ model) # model is the column created by gather predictions

# NOTE;
  # Note that the model that uses + has the same slope for each line, but 
  # different intercepts. The model that uses * has a different slope and 
  # intercept for each line.

# Which model is better??? Plot residuals!!!
sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, colour = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2)


# NOTE: LOOK FOR A PATTERN (heteroskedacity)
  # mod1 clearly there is a pattern in b & d ... model is missing something
  # mod2 has no obvious patterns of residuals ( BETTER)


### INTERACTIONS (2 CONT. VARIABLES) ---------------------
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5), # dont use every val of x, reg spaced grid
    x2 = seq_range(x2, 5)
  ) %>% 
  gather_predictions(mod1, mod2)
    #> # A tibble: 50 x 4
    #>   model    x1    x2   pred
    #>   <chr> <dbl> <dbl>  <dbl>
    #> 1 mod1   -1    -1    0.996
    #> 2 mod1   -1    -0.5 -0.395


# seq_range() ...
  # 1. pretty = TRUE
#seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)
        #> [1] 0.0 0.2 0.4 0.6 0.8 1.0
  # 2. trim = 0.1 will trim off 10% of the tail values. Useful if the variables have long tailed distribution and you want to focus on generating values near the center:
#x1 <- rcauchy(100)
#seq_range(x1, n = 5, trim = 0.10)
        #> [1] -13.841101  -8.709812  -3.578522   1.552767   6.684057

# VISAULIAZE SIM4 MODEL
ggplot(grid, aes(x1, x2)) +
  geom_tile(aes(fill = pred)) +
  facet_wrap(~ model)
    # hard to tell ....

# look at from both sides
ggplot(grid, aes(x1, pred, colour = x2, group = x2)) + 
  geom_line() +
  facet_wrap(~ model)
ggplot(grid, aes(x2, pred, colour = x1, group = x1)) + 
  geom_line() +
  facet_wrap(~ model)
    # ... still a bit tough, not easy visualizing 3 vars


### TRANSFORMATIONS --------------------

# can also perform tranforms (e.g. log(y) ~ sqrt(x1) + x2)

# if transform involves +, *, ^, or -  --> wrap in I()  ... I(x^2)

#  confused about what your model is doing, you can always use model_matrix()
  # to see exactly what equation lm() is fitting:

df <- tribble(
  ~y, ~x,
  1,  1,
  2,  2, 
  3,  3
)
model_matrix(df, y ~ x^2 + x)
    # # A tibble: 3 x 2
    # `     (Intercept)`     x
    #       <dbl>           <dbl>
    # 1             1     1
    # 2             1     2
    # 3             1     3
model_matrix(df, y ~ I(x^2) + x)
#> # A tibble: 3 x 3
#>   `(Intercept)` `I(x^2)`     x
#>           <dbl>    <dbl> <dbl>
#> 1             1        1     1
#> 2             1        4     2
#> 3             1        9     3


# fit a non linear model
sim5 <- tibble(
  x = seq(0, 3.5 * pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x))
)

ggplot(sim5, aes(x, y)) +
  geom_point()

# try 5 models .... splines::ns() for ploynomial
library(splines)

mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)

grid <- sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% 
  gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = "y")

ggplot(sim5, aes(x, y)) + 
  geom_point() +
  geom_line(data = grid, colour = "red") +
  facet_wrap(~ model)


# problem with every model: the model can never tell you if the behaviour is 
# true when you start extrapolating outside the range of the data that you have seen!!!



## EXERCISES -------

  # 1 - sim2 no intercept
mod2 <- lm(y ~ x, data = sim2)
mod2a <- lm(y ~ x - 1, data = sim2) # use -1 to drop intercept

# predictions end up the same
grid <- sim2 %>%
  data_grid(x) %>%
  spread_predictions(mod2, mod2a)
    # # A tibble: 4 x 3
    # x      mod2 mod2a
    # <chr> <dbl> <dbl>
    #   1 a      1.15  1.15
    # 2 b      8.12  8.12
    # 3 c      6.13  6.13
    # 4 d      1.91  1.91


# 2 - 
# Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4

# sim3
x3 <- model_matrix(y ~ x1 * x2, data = sim3)
x3

x4 <- model_matrix(y ~ x1 * x2, data = sim4)
x4


# 3 - 
  # Using the basic principles, convert the formulas in the following two models
  # into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

# x2 is categorical
levels(sim3$x2)

# convert to function
model_matrix_mod1 <- function(.data){
  mutate(.data,
         x2b = as.numeric(x2 == "b"),
         x2c = as.numeric(x2 == "c"),
         x2d = as.numeric(x2 == "d"),
         `(Intercept)` = 1
  ) %>% 
    select(`(Intercept)`,  x1, x2b, x2c, x2d)
      
}
model_matrix_mod1(sim3)



# 4 - 
# For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better
# job at removing patterns, but it's pretty subtle. Can you come up with a plot 
# to support my claim?
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>% 
  gather_residuals(mod1, mod2)

ggplot(grid, aes(x = resid, colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug()





# 23.5 MISSING VALUES -----------------------------------------------------

# modelling functions will drop any rows that contain missing values

# R's default behaviour is to silently drop them, but options(na.action = na.warn) 
# (run in the prerequisites), makes sure you get a warning.

df <- tribble(
  ~x, ~y,
  1, 2.2,
  2, NA,
  3, 3.5,
  4, 8.3,
  NA, 10
)

mod <- lm(y ~ x, data = df)

options(na.action = na.warn)
mod <- lm(y ~ x, data = df)
# Warning message:
# Dropping 2 rows with missing values 

# NUMBER OF OBS USED
nobs(mod)
# 3




# 23.6 OTHER MODEL FAMILIES -----------------------------------------------

Generalised linear models, e.g. stats::glm(). Linear models assume that the 
response is continuous and the error has a normal distribution. Generalised 
linear models extend linear models to include non-continuous responses 
(e.g. binary data or counts). They work by defining a distance metric based on 
the statistical idea of likelihood.

Generalised additive models, e.g. mgcv::gam(), extend generalised linear models 
to incorporate arbitrary smooth functions. That means you can write a formula 
like y ~ s(x) which becomes an equation like y = f(x) and let gam() estimate 
what that function is (subject to some smoothness constraints to make the 
                       problem tractable).

Penalised linear models, e.g. glmnet::glmnet(), add a penalty term to the
distance that penalises complex models (as defined by the distance between the 
parameter vector and the origin). This tends to make models that generalise 
better to new datasets from the same population.

Robust linear models, e.g. MASS::rlm(), tweak the distance to downweight points 
that are very far away. This makes them less sensitive to the presence of outliers,
at the cost of being not quite as good when there are no outliers.

Trees, e.g. rpart::rpart(), attack the problem in a completely different way 
than linear models. They fit a piece-wise constant model, splitting the data 
into progressively smaller and smaller pieces. Trees aren't terribly effective 
by themselves, but they are very powerful when used in aggregate by models like
random forests (e.g. randomForest::randomForest()) or gradient boosting machines 
(e.g. xgboost::xgboost.)
